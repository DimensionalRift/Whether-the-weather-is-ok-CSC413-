{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BKhrJPTWLywi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "import dataProcess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# should use a better way to convert nans to numbers\n",
        "def nan_to_zero(data):\n",
        "  for i in range(data.shape[0]):\n",
        "    for j in range(data.shape[1]):\n",
        "      for k in range(data.shape[2]):\n",
        "        if data[i, j, k].isnan:\n",
        "          data[i, j, k] = 0"
      ],
      "metadata": {
        "id": "C5u5zAw7L9Jq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "\n",
        "start = datetime(2024, 1, 3).date()\n",
        "end = datetime(2024, 4, 11).date()\n",
        "hourly_path ='weatherstats_toronto_hourly.csv'\n",
        "daily_path ='weatherstats_toronto_daily.csv'\n",
        "\n",
        "# data\n",
        "data_tensor_preprocess = dataProcess.dataToTensorHourly(hourly_path, separateByDay=False, missingThreshold=0.1, columnToDelete=['wind_dir', 'unixtime'], start=start, end=end)\n",
        "print(data_tensor_preprocess[0].shape)\n",
        "# for some reason going back one day does not work, but this does\n",
        "# need to go back one day so you're predicting the temperature for the next day\n",
        "data = data_tensor_preprocess[0].reshape(-1, 24, 13)[:-1]\n",
        "print(data.shape)\n",
        "nan_to_zero(data)\n",
        "\n",
        "# targets\n",
        "targets = dataProcess.dailyTargets(daily_path, start=datetime(2024, 1, 4).date())\n",
        "print(targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KddEbf0NL-s8",
        "outputId": "fe61bb75-e380-4b46-99f1-885d9e5ed7b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2376, 13])\n",
            "torch.Size([98, 24, 13])\n",
            "torch.Size([98])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherPredictorTransformer(nn.Module):\n",
        "  def __init__(self, input_size, d_model, nhead, num_encoder_layers, output_size):\n",
        "    super(WeatherPredictorTransformer, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Linear(input_size, d_model)\n",
        "    self.positional_encoding = self.create_sinusoidal_positional_encoding(24, d_model)\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "    self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
        "    self.output_linear = nn.Linear(d_model, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: [batch_size, 24, input_size]\n",
        "    x = self.embedding(x) # x: [batch_size, 24, d_model]\n",
        "    x += self.positional_encoding[:]\n",
        "    x = x.permute(1, 0, 2)  # x: [24, batch_size, d_model]\n",
        "    encoder_output = self.transformer_encoder(x)\n",
        "    prediction = self.output_linear(encoder_output[-1])\n",
        "    return prediction\n",
        "\n",
        "  def create_sinusoidal_positional_encoding(self, length, d_model):\n",
        "    PE = torch.zeros(length, d_model)\n",
        "    position = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
        "    PE[:, 0::2] = torch.sin(position * div_term)\n",
        "    PE[:, 1::2] = torch.cos(position * div_term)\n",
        "    return PE.unsqueeze(0)"
      ],
      "metadata": {
        "id": "LZPYotY98-U5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_for_transformer(train_fraction, valid_fraction, data, targets, batch_size):\n",
        "\n",
        "  train_dataset = TensorDataset(data, targets)\n",
        "  total_size = len(data)\n",
        "  train_split_point = int(total_size * train_fraction)\n",
        "  valid_split_point = train_split_point + int(total_size * valid_fraction)\n",
        "\n",
        "  train_dataset = TensorDataset(data[:train_split_point], targets[:train_split_point])\n",
        "  val_dataset = TensorDataset(data[train_split_point:valid_split_point],\n",
        "                              targets[train_split_point:valid_split_point])\n",
        "  test_dataset = TensorDataset(data[valid_split_point:], targets[valid_split_point:])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = process_data_for_transformer(0.8, 0.1, data, targets, 1)"
      ],
      "metadata": {
        "id": "n3xJAO6GMCfJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size=13\n",
        "d_model=512\n",
        "nhead=8\n",
        "num_encoder_layers=6\n",
        "output_size=1\n",
        "\n",
        "model = WeatherPredictorTransformer(input_size=input_size, d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, output_size=output_size)"
      ],
      "metadata": {
        "id": "L8-ni3blMFMR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, num_epochs, optimizer, criterion, train_loader, val_loader, device):\n",
        "  model = model.to(device)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for inputs, targets in train_loader:\n",
        "      inputs, targets = inputs.to(device).float(), targets.to(device).float()\n",
        "\n",
        "      assert not torch.isnan(inputs).any(), \"NaN found in inputs\"\n",
        "      assert not torch.isnan(targets).any(), \"NaN found in targets\"\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, targets in val_loader:\n",
        "        inputs, targets = inputs.to(device).float(), targets.to(device).float()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "train_model(model, 10, optimizer, criterion, train_loader, val_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArCQwfWUDO-a",
        "outputId": "6eec7002-75e0-4eb9-e57b-8eca97b6e51e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Training Loss: 29.1682\n",
            "Epoch [1/10], Validation Loss: 87.1941\n",
            "Epoch [2/10], Training Loss: 25.1657\n",
            "Epoch [2/10], Validation Loss: 90.9223\n",
            "Epoch [3/10], Training Loss: 24.7273\n",
            "Epoch [3/10], Validation Loss: 94.9328\n",
            "Epoch [4/10], Training Loss: 24.3803\n",
            "Epoch [4/10], Validation Loss: 98.2168\n",
            "Epoch [5/10], Training Loss: 24.0223\n",
            "Epoch [5/10], Validation Loss: 100.1720\n",
            "Epoch [6/10], Training Loss: 23.9162\n",
            "Epoch [6/10], Validation Loss: 101.7585\n",
            "Epoch [7/10], Training Loss: 23.6827\n",
            "Epoch [7/10], Validation Loss: 103.4656\n",
            "Epoch [8/10], Training Loss: 23.6566\n",
            "Epoch [8/10], Validation Loss: 105.1011\n",
            "Epoch [9/10], Training Loss: 23.4752\n",
            "Epoch [9/10], Validation Loss: 106.6330\n",
            "Epoch [10/10], Training Loss: 23.2037\n",
            "Epoch [10/10], Validation Loss: 107.2847\n"
          ]
        }
      ]
    }
  ]
}