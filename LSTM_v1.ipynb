{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LSTM,\n",
        "data pre-processing taken from Thomas code"
      ],
      "metadata": {
        "id": "SW5wZM5ahfPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Pm-GTpoheAS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import dataProcess\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nan_to_zero(data):\n",
        "  for i in range(data.shape[0]):\n",
        "    for j in range(data.shape[1]):\n",
        "      for k in range(data.shape[2]):\n",
        "        if data[i, j, k].isnan:\n",
        "          data[i, j, k] = 0"
      ],
      "metadata": {
        "id": "NwlSFDkHhe6Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "\n",
        "start = datetime(2024, 1, 3).date()\n",
        "end = datetime(2024, 4, 11).date()\n",
        "hourly_path ='weatherstats_toronto_hourly.csv'\n",
        "daily_path ='weatherstats_toronto_daily.csv'\n",
        "\n",
        "# data\n",
        "data_tensor_preprocess = dataProcess.dataToTensorHourly(hourly_path, separateByDay=False, missingThreshold=0.1, columnToDelete=['wind_dir', 'unixtime'], start=start, end=end)\n",
        "print(data_tensor_preprocess[0].shape)\n",
        "# for some reason going back one day does not work, but this does\n",
        "# need to go back one day so you're predicting the temperature for the next day\n",
        "data = data_tensor_preprocess[0].reshape(-1, 24, 13)[:-1]\n",
        "print(data.shape)\n",
        "nan_to_zero(data)\n",
        "\n",
        "# targets\n",
        "targets = dataProcess.dailyTargets(daily_path, start=datetime(2024, 1, 4).date())\n",
        "print(targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c0IMWpvhq3z",
        "outputId": "b73ef689-41cb-412f-b51e-8eb90ac5b641"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2376, 13])\n",
            "torch.Size([98, 24, 13])\n",
            "torch.Size([98])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define model\n",
        "class MyLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MyLSTM, self).__init__()\n",
        "        #recurrent layer\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
        "        #fully connected layer\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Initialize hidden state\n",
        "        # Forward pass through RNN\n",
        "        output, h_n = self.lstm(X)\n",
        "\n",
        "        #get output of the RNN at the last timestep\n",
        "        lstm_output_last = output[:, -1, :]\n",
        "        # Apply fully connected layer to the output of the last timestep\n",
        "        out = self.fc1(lstm_output_last)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "oSVjdNVWhuXs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#same method of splitting as in GRU\n",
        "def process_data_for_lstm(train_fraction, valid_fraction, data, targets, batch_size):\n",
        "\n",
        "  train_dataset = TensorDataset(data, targets)\n",
        "  total_size = len(data)\n",
        "  train_split_point = int(total_size * train_fraction)\n",
        "  valid_split_point = train_split_point + int(total_size * valid_fraction)\n",
        "\n",
        "  train_dataset = TensorDataset(data[:train_split_point], targets[:train_split_point])\n",
        "  val_dataset = TensorDataset(data[train_split_point:valid_split_point],\n",
        "                              targets[train_split_point:valid_split_point])\n",
        "  test_dataset = TensorDataset(data[valid_split_point:], targets[valid_split_point:])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = process_data_for_lstm(0.8, 0.1, data, targets, 1)"
      ],
      "metadata": {
        "id": "q_ZEfEo3kOJI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "input_size = 13  # number of weather features\n",
        "hidden_size = 50\n",
        "output_size = 1  # average temperature\n",
        "model = MyLSTM(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "XSXqZJM9kR96"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train_model(model, num_epochs, optimizer, criterion, train_loader, val_loader):\n",
        "\n",
        "  for inputs, targets in train_loader:\n",
        "    assert not torch.isnan(inputs).any(), \"NaN found in inputs\"\n",
        "    assert not torch.isnan(targets).any(), \"NaN found in targets\"\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "      inputs, targets = inputs.float(), targets.float()\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {total_loss / len(train_loader):.4f}')\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, targets in val_loader:\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {total_val_loss / len(val_loader):.4f}')\n",
        "\n",
        "train_model(model, num_epochs, optimizer, criterion, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE1I5Ovkkc6R",
        "outputId": "fbc27c69-b592-47a0-d5e2-ef8208aafbed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Training Loss: 22.1544\n",
            "Epoch [1/10], Validation Loss: 121.7494\n",
            "Epoch [2/10], Training Loss: 21.7572\n",
            "Epoch [2/10], Validation Loss: 119.7497\n",
            "Epoch [3/10], Training Loss: 21.8131\n",
            "Epoch [3/10], Validation Loss: 119.1978\n",
            "Epoch [4/10], Training Loss: 21.8258\n",
            "Epoch [4/10], Validation Loss: 119.0614\n",
            "Epoch [5/10], Training Loss: 21.8249\n",
            "Epoch [5/10], Validation Loss: 119.0571\n",
            "Epoch [6/10], Training Loss: 21.8196\n",
            "Epoch [6/10], Validation Loss: 119.0985\n",
            "Epoch [7/10], Training Loss: 21.8129\n",
            "Epoch [7/10], Validation Loss: 119.1535\n",
            "Epoch [8/10], Training Loss: 21.8062\n",
            "Epoch [8/10], Validation Loss: 119.2144\n",
            "Epoch [9/10], Training Loss: 21.7992\n",
            "Epoch [9/10], Validation Loss: 119.2745\n",
            "Epoch [10/10], Training Loss: 21.7919\n",
            "Epoch [10/10], Validation Loss: 119.3412\n"
          ]
        }
      ]
    }
  ]
}