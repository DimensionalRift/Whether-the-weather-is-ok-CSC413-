{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import dataProcess\n",
        "\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "mx_Fu4kjZfgu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# should use a better way to convert nans to numbers\n",
        "def nan_to_zero(data):\n",
        "  for i in range(data.shape[0]):\n",
        "    for j in range(data.shape[1]):\n",
        "      for k in range(data.shape[2]):\n",
        "        if data[i, j, k].isnan:\n",
        "          data[i, j, k] = 0"
      ],
      "metadata": {
        "id": "PS5sx417_eY5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "\n",
        "start = datetime(2024, 1, 3).date()\n",
        "end = datetime(2024, 4, 11).date()\n",
        "hourly_path ='weatherstats_toronto_hourly.csv'\n",
        "daily_path ='weatherstats_toronto_daily.csv'\n",
        "\n",
        "# data\n",
        "data_tensor_preprocess = dataProcess.dataToTensorHourly(hourly_path, separateByDay=False, missingThreshold=0.1, columnToDelete=['wind_dir', 'unixtime'], start=start, end=end)\n",
        "print(data_tensor_preprocess[0].shape)\n",
        "# for some reason going back one day does not work, but this does\n",
        "# need to go back one day so you're predicting the temperature for the next day\n",
        "data = data_tensor_preprocess[0].reshape(-1, 24, 13)[:-1]\n",
        "print(data.shape)\n",
        "nan_to_zero(data)\n",
        "\n",
        "# targets\n",
        "targets = dataProcess.dailyTargets(daily_path, start=datetime(2024, 1, 4).date())\n",
        "print(targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYmPNpjoD0Ds",
        "outputId": "f1a2cab0-f4a2-48d4-d957-83874be63b02"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2376, 13])\n",
            "torch.Size([98, 24, 13])\n",
            "torch.Size([98])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherPredictorGRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(WeatherPredictorGRU, self).__init__()\n",
        "    self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "    self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, hidden = self.gru(x)\n",
        "    last_hidden_state = hidden[-1]\n",
        "    prediction = self.linear(last_hidden_state)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "ImsvFznV0PjA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_for_rnn_gru(train_fraction, valid_fraction, data, targets, batch_size):\n",
        "\n",
        "  train_dataset = TensorDataset(data, targets)\n",
        "  total_size = len(data)\n",
        "  train_split_point = int(total_size * train_fraction)\n",
        "  valid_split_point = train_split_point + int(total_size * valid_fraction)\n",
        "\n",
        "  train_dataset = TensorDataset(data[:train_split_point], targets[:train_split_point])\n",
        "  val_dataset = TensorDataset(data[train_split_point:valid_split_point],\n",
        "                              targets[train_split_point:valid_split_point])\n",
        "  test_dataset = TensorDataset(data[valid_split_point:], targets[valid_split_point:])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = process_data_for_rnn_gru(0.8, 0.1, data, targets, 1)"
      ],
      "metadata": {
        "id": "2L9sP6zp0UD1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "input_size = 13  # number of weather features\n",
        "hidden_size = 50\n",
        "output_size = 1  # average temperature\n",
        "model = WeatherPredictorGRU(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "vVXtRjE_34q0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train_model(model, num_epochs, optimizer, criterion, train_loader, val_loader):\n",
        "\n",
        "  for inputs, targets in train_loader:\n",
        "    assert not torch.isnan(inputs).any(), \"NaN found in inputs\"\n",
        "    assert not torch.isnan(targets).any(), \"NaN found in targets\"\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "      inputs, targets = inputs.float(), targets.float()\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {total_loss / len(train_loader):.4f}')\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, targets in val_loader:\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {total_val_loss / len(val_loader):.4f}')\n",
        "\n",
        "train_model(model, num_epochs, optimizer, criterion, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWtHkzYq3_OY",
        "outputId": "92e389a1-1bf0-4bdc-c296-13e405fa3417"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Training Loss: 21.4687\n",
            "Epoch [1/10], Validation Loss: 127.7113\n",
            "Epoch [2/10], Training Loss: 21.2945\n",
            "Epoch [2/10], Validation Loss: 126.7376\n",
            "Epoch [3/10], Training Loss: 21.3012\n",
            "Epoch [3/10], Validation Loss: 126.1369\n",
            "Epoch [4/10], Training Loss: 21.3068\n",
            "Epoch [4/10], Validation Loss: 125.7649\n",
            "Epoch [5/10], Training Loss: 21.3102\n",
            "Epoch [5/10], Validation Loss: 125.4933\n",
            "Epoch [6/10], Training Loss: 21.3140\n",
            "Epoch [6/10], Validation Loss: 125.3842\n",
            "Epoch [7/10], Training Loss: 21.3151\n",
            "Epoch [7/10], Validation Loss: 125.3806\n",
            "Epoch [8/10], Training Loss: 21.3112\n",
            "Epoch [8/10], Validation Loss: 125.3392\n",
            "Epoch [9/10], Training Loss: 21.3107\n",
            "Epoch [9/10], Validation Loss: 125.3092\n",
            "Epoch [10/10], Training Loss: 21.3105\n",
            "Epoch [10/10], Validation Loss: 125.2953\n"
          ]
        }
      ]
    }
  ]
}